{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11c7f0410>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from collections import namedtuple\n",
    "from collections import deque\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "_ = plt.gray();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST(object):\n",
    "    image = namedtuple(\"image\", \"data,normalized_data,w,h,size\")\n",
    "    label = namedtuple(\"label\", \"label,encoded,size\")\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.train_image = self.read_image(\"train-images-idx3-ubyte\")\n",
    "        self.test_image = self.read_image(\"t10k-images-idx3-ubyte\")\n",
    "        self.train_label = self.read_label(\"train-labels-idx1-ubyte\")\n",
    "        self.test_label = self.read_label(\"t10k-labels-idx1-ubyte\")\n",
    "    \n",
    "    def as_int(self, bytes):\n",
    "        r = 0\n",
    "        for b in bytes:\n",
    "            r = (r << 8) | ord(b)\n",
    "        return r\n",
    "    \n",
    "    def read_label(self, filename):\n",
    "        with open(filename, \"rb\") as f:\n",
    "            self.as_int(f.read(4)) # magic number\n",
    "            size = self.as_int(f.read(4))\n",
    "            digits = np.array(map(lambda x: ord(x), f.read(-1)))\n",
    "            encoded = np.zeros([size, 10], dtype=np.float32)\n",
    "            encoded[np.arange(size),digits] = 1\n",
    "        return self.label(digits, encoded, size)\n",
    "    \n",
    "    def read_image(self, filename):\n",
    "        with open(filename, \"rb\") as f:\n",
    "            self.as_int(f.read(4)) # magic number\n",
    "            size = self.as_int(f.read(4))\n",
    "            w = self.as_int(f.read(4))\n",
    "            h = self.as_int(f.read(4))\n",
    "            data = np.array(map(lambda x: ord(x), f.read(-1)), dtype=np.float32).reshape(-1, w*h)\n",
    "            norm = (data - data.mean(axis=0)) / (data.std(axis=0) + 1e-5)\n",
    "        return self.image(data=data, normalized_data=norm, w=w, h=h, size=size)\n",
    "    \n",
    "    def _range_max(self, row, length):\n",
    "        queue = deque()\n",
    "        output = [0 for _ in xrange(len(row) - length + 1)]\n",
    "        length -= 1\n",
    "        for j, e in enumerate(row):\n",
    "            while(len(queue) != 0 and queue[0][0] <= e):\n",
    "                queue.popleft()\n",
    "            queue.appendleft((e, j))\n",
    "            if j < length:\n",
    "                continue\n",
    "            while(queue[-1][1] < (j - length)):\n",
    "                queue.pop()\n",
    "            \n",
    "            output[j - length] = queue[-1][0]\n",
    "            \n",
    "        return output\n",
    "        \n",
    "    def _grid_max(self, arr2d, grid=(4, 4)):\n",
    "        temp = np.array([self._range_max(arr1d, length=grid[0]) for arr1d in arr2d]).T\n",
    "        return np.array([self._range_max(arr1d, length=grid[1]) for arr1d in temp]).T\n",
    "    \n",
    "    def augment_feature(self, data, grid=(5, 5)):\n",
    "        \"\"\"\n",
    "        data is a 784xS size 2d array\n",
    "        each row is reshaped 28x28 image\n",
    "        lets calculate max in each grid\n",
    "        \"\"\"\n",
    "        imsize = 28\n",
    "        data_cube = data.reshape(data.shape[0], imsize, imsize)\n",
    "        return np.array([self._grid_max(matrix, grid) for matrix in data_cube])\n",
    "\n",
    "mnist = MNIST()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(self, size):\n",
    "    select = np.random.choice(self.train_image.size, size)\n",
    "    return (self.train_image.normalized_data[select,:], self.train_label.encoded[select,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.1030, loss=2.567219\n",
      "accuracy: 0.8519, loss=0.539366\n",
      "accuracy: 0.8853, loss=0.410318\n",
      "accuracy: 0.8988, loss=0.349932\n",
      "accuracy: 0.9113, loss=0.316727\n",
      "accuracy: 0.9149, loss=0.296560\n",
      "accuracy: 0.9187, loss=0.283441\n",
      "accuracy: 0.9193, loss=0.274365\n",
      "accuracy: 0.9256, loss=0.261219\n",
      "accuracy: 0.9268, loss=0.249327\n",
      "accuracy: 0.9302, loss=0.238319\n",
      "accuracy: 0.9306, loss=0.234933\n",
      "accuracy: 0.9339, loss=0.229801\n",
      "accuracy: 0.9329, loss=0.224741\n",
      "accuracy: 0.9376, loss=0.217748\n",
      "accuracy: 0.9394, loss=0.211483\n",
      "accuracy: 0.9408, loss=0.205580\n",
      "accuracy: 0.9406, loss=0.202732\n",
      "accuracy: 0.9427, loss=0.199952\n",
      "accuracy: 0.9427, loss=0.197214\n",
      "accuracy: 0.9438, loss=0.192560\n",
      "accuracy: 0.9453, loss=0.188550\n",
      "accuracy: 0.9467, loss=0.183512\n",
      "accuracy: 0.9483, loss=0.182057\n",
      "accuracy: 0.9481, loss=0.180020\n",
      "accuracy: 0.9482, loss=0.178750\n",
      "accuracy: 0.9494, loss=0.175059\n",
      "accuracy: 0.9504, loss=0.172079\n",
      "accuracy: 0.9507, loss=0.168491\n",
      "accuracy: 0.9525, loss=0.167392\n",
      "accuracy: 0.9523, loss=0.165757\n",
      "accuracy: 0.9535, loss=0.166463\n",
      "accuracy: 0.9526, loss=0.163730\n",
      "accuracy: 0.9547, loss=0.160119\n",
      "accuracy: 0.9546, loss=0.156529\n",
      "accuracy: 0.9549, loss=0.157248\n",
      "accuracy: 0.9559, loss=0.153435\n",
      "accuracy: 0.9567, loss=0.154491\n",
      "accuracy: 0.9569, loss=0.151548\n",
      "accuracy: 0.9574, loss=0.148680\n",
      "accuracy: 0.9575, loss=0.146791\n",
      "accuracy: 0.9589, loss=0.146115\n",
      "accuracy: 0.9587, loss=0.146740\n",
      "accuracy: 0.9588, loss=0.147514\n",
      "accuracy: 0.9594, loss=0.143377\n",
      "accuracy: 0.9608, loss=0.141788\n",
      "accuracy: 0.9594, loss=0.138841\n",
      "accuracy: 0.9616, loss=0.138080\n",
      "accuracy: 0.9604, loss=0.138480\n",
      "accuracy: 0.9617, loss=0.138660\n",
      "accuracy: 0.9622, loss=0.136578\n",
      "accuracy: 0.9623, loss=0.134829\n",
      "accuracy: 0.9617, loss=0.133217\n",
      "accuracy: 0.9629, loss=0.134072\n",
      "accuracy: 0.9626, loss=0.133645\n",
      "accuracy: 0.9638, loss=0.133822\n",
      "accuracy: 0.9647, loss=0.130933\n",
      "accuracy: 0.9647, loss=0.128303\n",
      "accuracy: 0.9647, loss=0.125961\n",
      "accuracy: 0.9646, loss=0.128566\n",
      "accuracy: 0.9645, loss=0.127110\n",
      "accuracy: 0.9654, loss=0.126846\n",
      "accuracy: 0.9665, loss=0.124367\n",
      "accuracy: 0.9670, loss=0.123531\n",
      "accuracy: 0.9654, loss=0.123430\n",
      "accuracy: 0.9661, loss=0.122798\n",
      "accuracy: 0.9666, loss=0.125696\n",
      "accuracy: 0.9663, loss=0.127294\n",
      "accuracy: 0.9672, loss=0.123269\n",
      "accuracy: 0.9682, loss=0.121283\n",
      "accuracy: 0.9676, loss=0.119507\n",
      "accuracy: 0.9692, loss=0.119250\n",
      "accuracy: 0.9685, loss=0.120764\n",
      "accuracy: 0.9686, loss=0.120258\n",
      "accuracy: 0.9685, loss=0.117390\n",
      "accuracy: 0.9698, loss=0.115416\n",
      "accuracy: 0.9691, loss=0.114987\n",
      "accuracy: 0.9698, loss=0.114521\n",
      "accuracy: 0.9696, loss=0.114838\n",
      "accuracy: 0.9691, loss=0.116942\n",
      "accuracy: 0.9690, loss=0.113791\n",
      "accuracy: 0.9700, loss=0.112998\n",
      "accuracy: 0.9702, loss=0.111694\n",
      "accuracy: 0.9702, loss=0.112880\n",
      "accuracy: 0.9697, loss=0.114053\n",
      "accuracy: 0.9706, loss=0.114273\n",
      "accuracy: 0.9694, loss=0.113990\n",
      "accuracy: 0.9703, loss=0.110800\n",
      "accuracy: 0.9702, loss=0.109700\n",
      "accuracy: 0.9706, loss=0.111972\n",
      "accuracy: 0.9702, loss=0.111571\n",
      "accuracy: 0.9707, loss=0.110904\n",
      "accuracy: 0.9715, loss=0.110153\n",
      "accuracy: 0.9707, loss=0.109011\n",
      "accuracy: 0.9709, loss=0.108059\n",
      "accuracy: 0.9712, loss=0.108485\n",
      "accuracy: 0.9712, loss=0.107452\n",
      "accuracy: 0.9709, loss=0.109715\n",
      "accuracy: 0.9714, loss=0.108530\n",
      "accuracy: 0.9718, loss=0.107923\n",
      "accuracy: 0.9727, loss=0.106686\n",
      "accuracy: 0.9714, loss=0.107477\n",
      "accuracy: 0.9717, loss=0.107692\n",
      "accuracy: 0.9721, loss=0.108082\n",
      "accuracy: 0.9719, loss=0.106840\n",
      "accuracy: 0.9721, loss=0.105219\n",
      "accuracy: 0.9731, loss=0.103424\n",
      "accuracy: 0.9722, loss=0.104125\n",
      "accuracy: 0.9726, loss=0.106817\n",
      "accuracy: 0.9720, loss=0.106493\n",
      "accuracy: 0.9724, loss=0.106114\n",
      "accuracy: 0.9725, loss=0.105612\n",
      "accuracy: 0.9729, loss=0.105145\n",
      "accuracy: 0.9730, loss=0.105402\n",
      "accuracy: 0.9724, loss=0.105829\n",
      "accuracy: 0.9719, loss=0.106650\n",
      "accuracy: 0.9727, loss=0.105759\n",
      "accuracy: 0.9720, loss=0.105226\n",
      "accuracy: 0.9729, loss=0.104573\n",
      "accuracy: 0.9729, loss=0.105372\n",
      "accuracy: 0.9729, loss=0.105218\n",
      "accuracy: 0.9722, loss=0.105091\n",
      "accuracy: 0.9726, loss=0.103095\n",
      "accuracy: 0.9737, loss=0.102949\n",
      "accuracy: 0.9739, loss=0.101906\n",
      "accuracy: 0.9735, loss=0.102276\n",
      "accuracy: 0.9729, loss=0.102752\n",
      "accuracy: 0.9727, loss=0.102640\n",
      "accuracy: 0.9739, loss=0.100952\n",
      "accuracy: 0.9730, loss=0.100079\n",
      "accuracy: 0.9747, loss=0.099697\n",
      "accuracy: 0.9738, loss=0.102007\n",
      "accuracy: 0.9736, loss=0.102492\n",
      "accuracy: 0.9737, loss=0.103187\n",
      "accuracy: 0.9736, loss=0.100572\n",
      "accuracy: 0.9736, loss=0.099851\n",
      "accuracy: 0.9741, loss=0.098870\n",
      "accuracy: 0.9742, loss=0.099141\n",
      "accuracy: 0.9746, loss=0.099767\n",
      "accuracy: 0.9737, loss=0.099913\n",
      "accuracy: 0.9744, loss=0.098430\n",
      "accuracy: 0.9740, loss=0.098135\n",
      "accuracy: 0.9747, loss=0.097615\n",
      "accuracy: 0.9740, loss=0.098002\n",
      "accuracy: 0.9746, loss=0.098908\n",
      "accuracy: 0.9746, loss=0.099134\n",
      "accuracy: 0.9747, loss=0.097909\n",
      "accuracy: 0.9746, loss=0.097034\n",
      "accuracy: 0.9748, loss=0.095816\n",
      "accuracy: 0.9750, loss=0.097358\n",
      "accuracy: 0.9754, loss=0.097339\n",
      "accuracy: 0.9740, loss=0.097115\n",
      "accuracy: 0.9752, loss=0.096904\n",
      "accuracy: 0.9749, loss=0.096179\n",
      "accuracy: 0.9752, loss=0.095002\n",
      "accuracy: 0.9750, loss=0.096505\n",
      "accuracy: 0.9754, loss=0.096510\n",
      "accuracy: 0.9747, loss=0.096819\n",
      "accuracy: 0.9750, loss=0.095949\n",
      "accuracy: 0.9746, loss=0.095334\n",
      "accuracy: 0.9749, loss=0.095119\n",
      "accuracy: 0.9749, loss=0.096326\n",
      "accuracy: 0.9753, loss=0.097921\n",
      "accuracy: 0.9753, loss=0.097467\n",
      "accuracy: 0.9753, loss=0.095629\n",
      "accuracy: 0.9751, loss=0.094671\n",
      "accuracy: 0.9760, loss=0.094618\n",
      "accuracy: 0.9756, loss=0.095080\n",
      "accuracy: 0.9754, loss=0.094971\n",
      "accuracy: 0.9757, loss=0.096718\n",
      "accuracy: 0.9751, loss=0.095297\n",
      "accuracy: 0.9753, loss=0.094200\n",
      "accuracy: 0.9758, loss=0.092553\n",
      "accuracy: 0.9748, loss=0.093992\n",
      "accuracy: 0.9752, loss=0.096168\n",
      "accuracy: 0.9758, loss=0.096287\n",
      "accuracy: 0.9753, loss=0.095288\n",
      "accuracy: 0.9760, loss=0.095350\n",
      "accuracy: 0.9769, loss=0.092684\n",
      "accuracy: 0.9755, loss=0.093433\n",
      "accuracy: 0.9761, loss=0.093811\n",
      "accuracy: 0.9753, loss=0.094334\n",
      "accuracy: 0.9758, loss=0.094116\n",
      "accuracy: 0.9757, loss=0.094506\n",
      "accuracy: 0.9766, loss=0.090801\n",
      "accuracy: 0.9762, loss=0.092298\n",
      "accuracy: 0.9760, loss=0.094621\n",
      "accuracy: 0.9759, loss=0.093611\n",
      "accuracy: 0.9757, loss=0.092984\n",
      "accuracy: 0.9759, loss=0.091856\n",
      "accuracy: 0.9759, loss=0.091224\n",
      "accuracy: 0.9763, loss=0.091563\n",
      "accuracy: 0.9760, loss=0.093065\n",
      "accuracy: 0.9764, loss=0.092638\n",
      "accuracy: 0.9764, loss=0.092193\n",
      "accuracy: 0.9764, loss=0.091666\n",
      "accuracy: 0.9765, loss=0.090371\n",
      "accuracy: 0.9762, loss=0.092404\n",
      "accuracy: 0.9763, loss=0.093120\n",
      "accuracy: 0.9761, loss=0.092770\n",
      "0.9762\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "def variable_summaries(var, name):\n",
    "    with tf.name_scope('summaries/' + name):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)\n",
    "\n",
    "x = tf.placeholder(tf.float64, [None, 784])\n",
    "keep_prob = tf.placeholder(dtype=tf.float64)\n",
    "dropped = tf.nn.dropout(x, keep_prob)\n",
    "\n",
    "with tf.variable_scope(\"Layer-1\"):\n",
    "    W1 = tf.get_variable(\"W1\", shape=[784, 400], dtype=tf.float64, initializer=tf.glorot_uniform_initializer())\n",
    "    b1 = tf.get_variable(\"b1\", shape=[400], dtype=tf.float64, initializer=tf.zeros_initializer())\n",
    "    y1 = tf.matmul(dropped, W1) + b1\n",
    "    variable_summaries(W1, \"Weights\")\n",
    "    variable_summaries(b1, \"Biases\")\n",
    "\n",
    "with tf.variable_scope(\"Layer-1-Activation\"):\n",
    "    y1_ = tf.nn.relu(y1)\n",
    "    variable_summaries(y1_, \"Relu-Activations\")\n",
    "\n",
    "with tf.variable_scope(\"Layer-2\"):\n",
    "    W = tf.get_variable(\"W2\", shape=[400, 10], dtype=tf.float64, initializer=tf.glorot_uniform_initializer())\n",
    "    b = tf.Variable(tf.zeros([10], dtype=tf.float64), name=\"bias\")\n",
    "    y = tf.matmul(y1_, W) + b\n",
    "    variable_summaries(W, \"Weights\")\n",
    "    variable_summaries(b, \"Biases\")\n",
    "    variable_summaries(y, \"WX_plus_b\")\n",
    "    y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "with tf.variable_scope(\"Loss\"):\n",
    "    cross_entropy = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y), name=\"Cross-Entropy-Loss\")\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.02).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "merged = tf.summary.merge_all()\n",
    "with tf.Session() as sess:\n",
    "    train_writer = tf.summary.FileWriter('train', sess.graph)\n",
    "    tf.global_variables_initializer().run()\n",
    "    # Train\n",
    "    batch_size = 100\n",
    "    iterations = 20000\n",
    "    for i in range(iterations):\n",
    "        batch_xs, batch_ys = next_batch(mnist, batch_size)\n",
    "        sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys, keep_prob:0.7})\n",
    "        \n",
    "        if (batch_size * i % 10000) == 0:\n",
    "            batch_xs, batch_ys = mnist.test_image.normalized_data, mnist.test_label.encoded\n",
    "            summary, acc_, loss_ = sess.run([merged, accuracy, cross_entropy], \n",
    "                                            feed_dict={x: batch_xs, y_: batch_ys, keep_prob:1.0})\n",
    "            train_writer.add_summary(summary, global_step=i * batch_size / 10000)\n",
    "            print \"accuracy: %0.4f, loss=%0.6f\" % (acc_, loss_)\n",
    "        \n",
    "    # Test trained model\n",
    "    print(sess.run(accuracy, feed_dict={x: mnist.test_image.normalized_data,\n",
    "                                      y_: mnist.test_label.encoded, keep_prob:1.0}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
